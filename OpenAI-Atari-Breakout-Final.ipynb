{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.0 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is from Deep Mind. TODO:  update comments and compare with paper.\n",
    "def cnn_model():\n",
    "    # With the functional API we need to define the inputs.\n",
    "    frames_input = layers.Input(ATARI_IMAGE_SHAPE, name='frames')\n",
    "    actions_input = layers.Input((ACTION_OPTION_COUNT,), name='action_mask')\n",
    "\n",
    "    # Assuming that the input frames are still encoded from 0 to 255. Transforming to [0, 1].\n",
    "    normalized = layers.Lambda(lambda x: x / 255.0, name='normalization')(frames_input)\n",
    "\n",
    "    # \"The first hidden layer convolves 16 8×8 filters with stride 4 with the input image and applies a rectifier nonlinearity.\"\n",
    "    conv_1 = layers.convolutional.Conv2D(\n",
    "        16, (8, 8), strides=(4, 4), activation='relu'\n",
    "    )(normalized)\n",
    "    # \"The second hidden layer convolves 32 4×4 filters with stride 2, again followed by a rectifier nonlinearity.\"\n",
    "    conv_2 = layers.convolutional.Conv2D(\n",
    "        32, (4, 4), strides=(2, 2), activation='relu'\n",
    "    )(conv_1)\n",
    "    # Flattening the second convolutional layer.\n",
    "    conv_flattened = layers.core.Flatten()(conv_2)\n",
    "    # \"The final hidden layer is fully-connected and consists of 256 rectifier units.\"\n",
    "    hidden = layers.Dense(256, activation='relu')(conv_flattened)\n",
    "    # \"The output layer is a fully-connected linear layer with a single output for each valid action.\"\n",
    "    output = layers.Dense(ACTION_OPTION_COUNT)(hidden)\n",
    "    # Finally, we multiply the output by the mask!\n",
    "    filtered_output = layers.Multiply(name='QValue')([output, actions_input])\n",
    "\n",
    "    model = Model(inputs=[frames_input, actions_input], outputs=filtered_output)\n",
    "    model.summary()\n",
    "    optimizer = RMSprop(lr=LEARNING_RATE, rho=0.95, epsilon=0.01)\n",
    "    # model.compile(optimizer, loss='mse')\n",
    "    # to changed model weights more slowly, uses MSE for low values and MAE(Mean Absolute Error) for large values\n",
    "    model.compile(optimizer, loss=huber_loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(observe):\n",
    "    processed_observe = np.uint8(\n",
    "        resize(rgb2gray(observe), (84, 84), mode='constant') * 255)\n",
    "    return processed_observe\n",
    "\n",
    "def get_log_dir():\n",
    "    curr_time = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    record_dir = \"{}/run-{}-log\".format(TRAIN_DIR, curr_time)\n",
    "    \n",
    "def init_file_writer_to_local_dir():\n",
    "    return tf.summary.FileWriter(get_log_dir(), tf.get_default_graph())\n",
    "\n",
    "def init_history(observe):\n",
    "    # At start of game, there is no preceding frame\n",
    "    # So just copy initial states to make history\n",
    "    state = pre_processing(observe)\n",
    "    # state = preprocess(observe)\n",
    "    history = np.stack((state, state, state, state), axis=2)\n",
    "    history = np.reshape([history], (1, 84, 84, 4))\n",
    "    return history\n",
    "\n",
    "def total_init_history(env):\n",
    "    observe = env.reset()\n",
    "    for _ in range(random.randint(1, NUM_OBSERVABLE_STEPS)):\n",
    "        observe, _, _, _ = env.step(1)\n",
    "    # At start of game, there is no preceding frame\n",
    "    # So just copy initial states to make history\n",
    "    state = pre_processing(observe)\n",
    "    history = np.stack((state, state, state, state), axis=2)\n",
    "    history = np.reshape([history], (1, 84, 84, 4))\n",
    "    return history\n",
    "\n",
    "def init_model_clone(model):\n",
    "    # Copy model since actual model weights will get updated later TODO.  when?\n",
    "    # Clone model using keras api function.\n",
    "    model_clone = clone_model(model)\n",
    "    # Clone model weights to new model separately\n",
    "    model_clone.set_weights(model.get_weights())\n",
    "    return model_clone\n",
    "\n",
    "def init_config():\n",
    "    env = gym.make('BreakoutDeterministic-v4')\n",
    "    # Deque is imported from collections.  Set to a finite size.  New memory will overwrite old.\n",
    "    memory = deque(maxlen=400000)\n",
    "    # init\n",
    "    epsilon = 1.0\n",
    "    # Calc decay rate.\n",
    "    epsilon_decay = ((1.0 - 0.1) / 1000000)\n",
    "    total_steps = 0\n",
    "    # Init at 0.  player_game -> step_count\n",
    "    player_game = 0\n",
    "    return (env, memory, epsilon, epsilon_decay, total_steps, player_game)\n",
    "\n",
    "def init_game_config():\n",
    "    done = False\n",
    "    dead = False\n",
    "    game_step = 0\n",
    "    game_score = 0\n",
    "    game_lives = 5\n",
    "    game_loss = 0.0\n",
    "    return (done, dead, game_step, game_score, game_lives, game_loss)\n",
    "\n",
    "def init_batch_matrix():\n",
    "    return np.zeros((BATCH_SIZE, ATARI_IMAGE_SHAPE[0], ATARI_IMAGE_SHAPE[1], ATARI_IMAGE_SHAPE[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(y, q_value):\n",
    "    error = K.abs(y - q_value)\n",
    "    quadratic_part = K.clip(error, 0.0, 1.0)\n",
    "    linear_part = error - quadratic_part\n",
    "    loss = K.mean(0.5 * K.square(quadratic_part) + linear_part)\n",
    "    return loss\n",
    "\n",
    "def find_state_and_history(observed_state, history):\n",
    "    next_state = pre_processing(observed_state)\n",
    "    # next_state = preprocess(observed_state)\n",
    "    next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "    next_history = np.append(next_state, history)\n",
    "    return (next_state, next_history)\n",
    "\n",
    "def get_one_hot_encoded_action_mask():\n",
    "    np.ones(ACTION_OPTION_COUNT).reshape(1, ACTION_OPTION_COUNT)\n",
    "\n",
    "# Action is random if it is an observed state or if by chance based on the epsilon threshold, it is.\n",
    "# If action is not random it gets generated from the current model based on history data to this point.\n",
    "# I select the best action from this result.\n",
    "def get_action(history, epsilon, step, model):\n",
    "    is_in_observed_state = (step <= NUM_OBSERVABLE_STEPS)\n",
    "    rand_choice_is_under_epsilon_threshold = (np.random.rand() <= epsilon)\n",
    "    if rand_choice_is_under_epsilon_threshold or is_in_observed_state:\n",
    "        return random.randrange(ACTION_OPTION_COUNT)\n",
    "    else:\n",
    "        q_value = model.predict([history, get_one_hot_encoded_action_mask()()])\n",
    "    # Offset for 0 indexing of one-hot encoding array location of values\n",
    "    return np.argmax(q_value[0]) + 1\n",
    "\n",
    "def update_epsilon(total_steps, epsilon, epsilon_decay):\n",
    "    training = (total_steps > NUM_OBSERVABLE_STEPS)\n",
    "    epsilon_declining = epsilon > 0.1\n",
    "    if epsilon_declining and training:\n",
    "        epsilon -= epsilon_decay\n",
    "    return epsilon\n",
    "\n",
    "def breakout_from_memory(memory):\n",
    "    training_batch = random.sample(memory, BATCH_SIZE)\n",
    "    \n",
    "    history = init_batch_matrix()\n",
    "    next_history = init_batch_matrix()\n",
    "    action, reward, dead = [], [], []\n",
    "\n",
    "    # Memory is stored in: indices 0 = history, 1 = action, 2 = reward, 3 = next_history, 4 = dead\n",
    "    for index, val in enumerate(training_batch):\n",
    "        print('val: ', val)\n",
    "        print('history index: ', history[index])\n",
    "        history[index] = val[0]\n",
    "        next_history[index] = val[3]\n",
    "        action.append(val[1])\n",
    "        reward.append(val[2])\n",
    "        dead.append(val[4])\n",
    "        \n",
    "    return (history, next_history, action, reward, dead)\n",
    "\n",
    "# 210*160*3(color) --> 84*84(mono)\n",
    "# float --> integer (to reduce the size of replay memory)\n",
    "def to_grayscale(img):\n",
    "    return np.mean(img, axis=2).astype(np.uint8)\n",
    "\n",
    "def downsample(img):\n",
    "    return img[::2, ::2]\n",
    "\n",
    "def preprocess(img):\n",
    "    return to_grayscale(downsample(img))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging Method\n",
    "It is not necessary to run this but the training is so long it is useful to check in on how it is performing.\n",
    "Some iterative logging function should be run in case the model quits while the programmer is sleeping or something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: delete.\n",
    "def maybe_log_stuff(model, total_steps, player_game, score, loss, step, memory, file_writer):\n",
    "    if player_game % 100 == 0:\n",
    "        print('player_game: {}, score: {}, total_steps: {}, avg loss: {}, step: {}, memory length: {}'\n",
    "              .format(player_game, score, total_steps, loss / float(step), step, len(memory)))\n",
    "\n",
    "    if player_game % 1000 == 0 or (player_game + 1) == NUM_TURNS:\n",
    "        now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "        file_name = \"training_{}.h5\".format(now)\n",
    "        model_path = os.path.join(TRAIN_DIR, file_name)\n",
    "        model.save(model_path)\n",
    "\n",
    "    # Add user custom data to TensorBoard\n",
    "    loss_summary = tf.Summary(\n",
    "        value=[tf.Summary.Value(tag=\"loss\", simple_value=loss / float(step))])\n",
    "    file_writer.add_summary(loss_summary, global_step=player_game)\n",
    "\n",
    "    score_summary = tf.Summary(\n",
    "        value=[tf.Summary.Value(tag=\"score\", simple_value=score)])\n",
    "    file_writer.add_summary(score_summary, global_step=player_game)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Network Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_memory_batch(memory, model):\n",
    "    mini_batch = random.sample(memory, 32)\n",
    "    history = np.zeros((32, ATARI_SHAPE[0],\n",
    "                        ATARI_SHAPE[1], ATARI_SHAPE[2]))\n",
    "    next_history = np.zeros((32, ATARI_SHAPE[0],\n",
    "                             ATARI_SHAPE[1], ATARI_SHAPE[2]))\n",
    "    target = np.zeros((32,))\n",
    "    action, reward, dead = [], [], []\n",
    "\n",
    "    # catj: memory is stored in:\n",
    "        # catj: for indices 0 = history, 1 = action, 2 = reward, 3 = next_history, 4 = dead\n",
    "    for idx, val in enumerate(mini_batch):\n",
    "        history[idx] = val[0]\n",
    "        next_history[idx] = val[3]\n",
    "        action.append(val[1])\n",
    "        reward.append(val[2])\n",
    "        dead.append(val[4])\n",
    "\n",
    "    actions_mask = np.ones((32, ACTION_SIZE))\n",
    "    # catj: predict for each action since mask is all 1s.\n",
    "    next_Q_values = model.predict([next_history, actions_mask])\n",
    "\n",
    "    # like Q Learning, get maximum Q value at s'\n",
    "    # But from target model\n",
    "    for i in range(32):\n",
    "        if dead[i]:\n",
    "            target[i] = -1\n",
    "            # target[i] = reward[i]\n",
    "        else:\n",
    "            # catj: Q(s, a) = r + gamma * max(Q(s', a'))\n",
    "            target[i] = reward[i] + f_gamma * np.amax(next_Q_values[i])\n",
    "\n",
    "    # catj get an action for each possible reward.\n",
    "    action_one_hot = get_one_hot(action, ACTION_SIZE)\n",
    "    # catj map each action to reward\n",
    "    target_one_hot = action_one_hot * target[:, None]\n",
    "\n",
    "    #tb_callback = TensorBoard(log_dir=log_dir, histogram_freq=0,\n",
    "    #                          write_graph=True, write_images=False)\n",
    "\n",
    "    h = model.fit(\n",
    "        [history, action_one_hot], target_one_hot, epochs=1,\n",
    "        batch_size=32, verbose=0)\n",
    "        #batch_size=FLAGS.batch_size, verbose=0, callbacks=[tb_callback])\n",
    "\n",
    "    #if h.history['loss'][0] > 10.0:\n",
    "    #    print('too large')\n",
    "\n",
    "    return h.history['loss'][0]\n",
    "\n",
    "# This function parents the deep Q Network if the model has enough memory for batch training.\n",
    "def deep_q_iteration_training(memory, total_steps, model_clone, model):\n",
    "    has_reached_training_threshold = (total_steps > NUM_OBSERVABLE_STEPS)\n",
    "    refresh_weights_threshold_met = (total_steps % MODEL_WEIGHTS_REFRESH_THRESOLD == 0)\n",
    "    # When the model has sufficiently recorded enough memory for training, start batch training.\n",
    "    if has_reached_training_threshold is True:\n",
    "        model_loss = train_memory_batch(memory, model)\n",
    "        # Weights on the model clone get piped through so they only get updated as often as \n",
    "        # the treshold dictates the cycle update them.\n",
    "        if refresh_weights_threshold_met:\n",
    "            model_clone.set_weights(model.get_weights())\n",
    "        return model_loss\n",
    "    return 0\n",
    "\n",
    "def get_next_history(observed_state, history):\n",
    "    # Keep state of MDP state.\n",
    "    next_state, next_history = find_state_and_history(observed_state, history)\n",
    "    return next_history\n",
    "\n",
    "def update_game_lifecycle(game_lives, info):\n",
    "    game_dead = game_lives > info['ale.lives']\n",
    "    game_lives = info['ale.lives']\n",
    "    return (game_dead, game_lives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostly this function keeps track of system states, memory, and flags\n",
    "# It provides the opportunity to create logs for debugging\n",
    "# Most importantly it takes an action and updates a score.\n",
    "# It runs training on the model if all observation has been done.  This is Deep Q Learning.\n",
    "\n",
    "def train():\n",
    "    env, memory, epsilon, epsilon_decay, total_steps, player_games = init_config()\n",
    "    # Get a copy of the cnn model with the architecture defined in a separate function.\n",
    "    model = cnn_model()\n",
    "    # Initialize file writer.  \n",
    "    # This is just used for logging and storing the model iteratively to preserve work.\n",
    "    file_writer = init_file_writer_to_local_dir()\n",
    "    # The main model gets used in the Q learning training, and based on updated weights, \n",
    "    # then also updates the model clone.\n",
    "    model_clone = init_model_clone(model)\n",
    "    # This is just a loop to cover the range of the global number of games played.\n",
    "    # The player games number is kept visible to the program for logging purposes.\n",
    "    while player_games < NUM_TURNS:\n",
    "        \n",
    "        game_done, game_dead, game_step, game_score, game_lives, game_loss = init_game_config()\n",
    "        observe = env.reset()\n",
    "\n",
    "        for _ in range(random.randint(1, 30)):\n",
    "            observe, _, _, _ = env.step(1)\n",
    "        # At start of the game, there is no preceding frame\n",
    "        # So just copy initial states to make history\n",
    "        state = pre_processing(observe)\n",
    "        # state = preprocess(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        \n",
    "        while not game_done:\n",
    "            # Epsilon has to decay a tiny bit with each iteration in the annealing method.\n",
    "            epsilon = update_epsilon(total_steps, epsilon, epsilon_decay)\n",
    "\n",
    "            # Get an action\n",
    "            action = get_action(history, epsilon, total_steps, model_clone)\n",
    "\n",
    "            # Take a step in the game\n",
    "            observed_state, reward, game_done, info = env.step(action)\n",
    "            \n",
    "            # Update score based on agent action.\n",
    "            # Move reward to the poles of 1 or -1 per the deep mind paper's suggestion\n",
    "            game_reward = np.clip(reward, -1., 1.)\n",
    "            game_score += game_reward\n",
    "            \n",
    "            # Deep Q learning begins if the observational state is complete.\n",
    "            model_loss = deep_q_iteration_training(memory, total_steps, model_clone, model)\n",
    "            game_loss += model_loss\n",
    "            \n",
    "            # Preprocess state to reduce image size, grayscale, and merge it with the history.\n",
    "            # next history is the result of data augmentation and merge.\n",
    "            # next_history = get_next_history(observed_state, history)\n",
    "            next_state = pre_processing(observed_state)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history)\n",
    "            \n",
    "            game_dead, game_lives = update_game_lifecycle(game_lives, info)\n",
    "            # Store history, and result of action, especially the action and reward\n",
    "            # This is used in the batch sampling to train the model during the training phase.\n",
    "            # This step is always used in the observing phase\n",
    "            memory.append((history, action, reward, next_history, game_dead))\n",
    "            \n",
    "            if not game_dead:\n",
    "                # Update history to include the state if the agent didn't die.\n",
    "                history = next_history\n",
    "\n",
    "            # Update counts and state flags.\n",
    "            dead = False\n",
    "            # These are used more of less for logging and aren't too important to the system.\n",
    "            total_steps += 1\n",
    "            game_step += 1\n",
    "            \n",
    "            if game_done:\n",
    "                maybe_log_stuff(model, total_steps, player_games, game_score, game_loss, game_step, memory, file_writer)\n",
    "                player_games += 1\n",
    "\n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    env = gym.make('BreakoutDeterministic-v4')\n",
    "\n",
    "    episode_number = 0\n",
    "    epsilon = 0.001\n",
    "    global_step = NUM_OBSERVABLE_STEPS + 1\n",
    "    model = load_model(RESTORE_FILE_PATH, custom_objects={'huber_loss': huber_loss})\n",
    "\n",
    "    while player_games < NUM_TURNS:\n",
    "        # init variables\n",
    "        game_done, game_dead, game_step, game_score, game_lives, game_loss = init_game_config()\n",
    "        \n",
    "        observe = env.reset()\n",
    "\n",
    "        # Copy in initial states to amount to initial four frame history\n",
    "        observe, _, _, _ = env.step(1)\n",
    "        history = init_history(observe)\n",
    "        while not done:\n",
    "            if RENDER:\n",
    "                env.render()\n",
    "                time.sleep(0.01)\n",
    "\n",
    "            # Get action.\n",
    "            action = get_action(history, epsilon, player_games, model)\n",
    "\n",
    "            # Take a step in the game.\n",
    "            observed_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Preprocess state to reduce image size, grayscale, and merge it with the history.\n",
    "            # next history is the result of data augmentation and merge.\n",
    "            next_state, next_history = find_state_and_history(observed_state)\n",
    "\n",
    "            game_dead, game_lives = update_game_lifecycle(game_lives, info)\n",
    "\n",
    "            # move reward to the poles of 1 or -1 per the deep mind paper's suggestion\n",
    "            game_reward = np.clip(reward, -1., 1.)\n",
    "\n",
    "            game_score += game_reward\n",
    "\n",
    "            if not game_dead:\n",
    "                # Update history to include the state if the agent didn't die.\n",
    "                history = next_history\n",
    "\n",
    "            # Update counts and state flags.\n",
    "            dead = False\n",
    "            # This is used more of less for logging and aren't too important to the system.\n",
    "            game_step += 1\n",
    "            \n",
    "            if game_done:\n",
    "                player_games += 1\n",
    "                print('episode: {}, score: {}'.format(player_games, game_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "\n",
    "from collections import deque\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import time\n",
    "from keras.models import load_model\n",
    "from keras.models import clone_model\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'openai_breakout_training_storage'\n",
    "RESTORE_FILE_PATH = '/Users/catherinejohnson/Downloads/Project Announcement-20201115/openai_breakout_training_storage/training_20201128034747.h5'\n",
    "NUM_TURNS = 100000\n",
    "NUM_OBSERVABLE_STEPS = 50000\n",
    "MODEL_WEIGHTS_REFRESH_THRESOLD = 10000\n",
    "INIT_NO_OP_STEPS = 30\n",
    "REGULATION_SCALE = 0.01\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.00025\n",
    "GAMMA = 0.99\n",
    "RESUME = False\n",
    "RENDER = False\n",
    "ATARI_IMAGE_SHAPE = (84, 84, 4)\n",
    "ATARI_SHAPE = (84, 84, 4)\n",
    "ACTION_OPTION_COUNT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "frames (InputLayer)             (None, 84, 84, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "normalization (Lambda)          (None, 84, 84, 4)    0           frames[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 20, 20, 16)   4112        normalization[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 9, 9, 32)     8224        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 2592)         0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 256)          663808      flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 3)            771         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "action_mask (InputLayer)        (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "QValue (Multiply)               (None, 3)            0           dense_8[0][0]                    \n",
      "                                                                 action_mask[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 676,915\n",
      "Trainable params: 676,915\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "player_game: 0, score: 2.0, total_steps: 185, avg loss: 0.0, step: 185, memory length: 185\n",
      "player_game: 100, score: 0.0, total_steps: 18579, avg loss: 0.0, step: 112, memory length: 18579\n",
      "player_game: 200, score: 2.0, total_steps: 37748, avg loss: 0.0, step: 195, memory length: 37748\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-2d15453a7a23>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# Get an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_clone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# Take a step in the game\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-57f1ff7c3a8e>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(history, epsilon, step, model)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mACTION_OPTION_COUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mq_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_one_hot_encoded_action_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Offset for 0 indexing of one-hot encoding array location of values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
